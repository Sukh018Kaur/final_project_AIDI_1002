{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Title: Potato Crop Disease Classification\n",
        "\n",
        "#### Group Member Names : Sukhjeet Kaur (200575307)\n",
        "\n"
      ],
      "metadata": {
        "id": "rMjEzLj-rF72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INTRODUCTION:\n",
        "*********************************************************************************************************************\n",
        "#### AIM : The aim of this project is to replicate and extend the research presented in the paper \"Soybean disease identification using original field images and transfer learning with convolutional neural networks\" by adapting the methodology for a different crop disease dataset. Specifically, we have used a potato dataset to evaluate the performance of the transfer learning approach in identifying potato crop diseases.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### Github Repo: The code and resources for the project are available in the GitHub repository: https://github.com/nzb0054/RoboCrop-CNN-WebApp\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### DESCRIPTION OF PAPER: The paper \"Soybean disease identification using original field images and transfer learning with convolutional neural networks\" presents a methodology for identifying soybean diseases using convolutional neural networks (CNNs) with transfer learning. The authors employed pre-trained CNN models to leverage learned features from large-scale datasets and fine-tuned them for soybean disease classification.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### PROBLEM STATEMENT : Identifying crop diseases accurately is crucial for effective crop management and yield optimization. Traditional methods of disease identification are labor-intensive and often lack precision. The problem is to enhance disease identification accuracy using image-based methods, specifically focusing on potato crop diseases in this study.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### CONTEXT OF THE PROBLEM: Potato crops are susceptible to various diseases that can significantly impact yield and quality. Early and accurate disease detection can help in timely intervention, reducing the potential loss. Utilizing image-based classification techniques with transfer learning provides an opportunity to improve the efficiency and accuracy of disease identification.\n",
        "\n"
      ],
      "metadata": {
        "id": "LuJpaCg6rF-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clone the GitHub Repository:"
      ],
      "metadata": {
        "id": "HE-IvOTsnlfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nzb0054/RoboCrop-CNN-WebApp.git\n",
        "%cd RoboCrop-CNN-WebApp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iROEWEj9cepF",
        "outputId": "2ba60835-333c-443d-877b-65b05d8c42eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RoboCrop-CNN-WebApp'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 21), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (44/44), 11.25 MiB | 10.24 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "/content/RoboCrop-CNN-WebApp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image size and batch size initialization"
      ],
      "metadata": {
        "id": "ksW_bqaOnrTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_height, img_width = (150, 150)\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "Sh2oKQT_hdvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Augmentation"
      ],
      "metadata": {
        "id": "l3eJPdyhn5jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    fill_mode=\"nearest\",\n",
        "    brightness_range=[0.9, 1.1],\n",
        "    rotation_range=30,\n",
        "    vertical_flip=True,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.05,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/PLD_3_Classes_256 2/Training',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "valid_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/PLD_3_Classes_256 2/Validation',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/PLD_3_Classes_256 2/Testing',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=1,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYQcOy73jfdx",
        "outputId": "c7f05c9f-f235-4b40-8ef1-c99342022879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 627 images belonging to 3 classes.\n",
            "Found 20 images belonging to 3 classes.\n",
            "Found 20 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the Base Model and Build the Model:"
      ],
      "metadata": {
        "id": "-jJ1Kl0Wn_FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "base_model = DenseNet201(include_top=False, weights='imagenet')\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "-hCQ06oyj5B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile the Model"
      ],
      "metadata": {
        "id": "FywzcY6poax8"
      }
    },
    {
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "INqJN5LbkEpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Model for 30 epochs"
      ],
      "metadata": {
        "id": "8dSSrfB6oiKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "log_csv = CSVLogger('my_logs_robocrop.csv', separator=',', append=False)\n",
        "callbacks_list = [log_csv]\n",
        "\n",
        "model.fit(train_generator,\n",
        "          epochs=30,\n",
        "          validation_data=valid_generator,\n",
        "          callbacks=callbacks_list,\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK6eIQZTkI-y",
        "outputId": "eb178e17-2b16-4a0a-adad-861151a6f9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3s/step - accuracy: 0.7527 - loss: 0.6597 - val_accuracy: 0.4000 - val_loss: 1.5106\n",
            "Epoch 2/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 229ms/step - accuracy: 0.9939 - loss: 0.2169 - val_accuracy: 0.4000 - val_loss: 2.1290\n",
            "Epoch 3/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 254ms/step - accuracy: 0.9943 - loss: 0.0923 - val_accuracy: 0.4000 - val_loss: 2.6008\n",
            "Epoch 4/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - accuracy: 0.9982 - loss: 0.0662 - val_accuracy: 0.4000 - val_loss: 2.8003\n",
            "Epoch 5/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 238ms/step - accuracy: 1.0000 - loss: 0.0459 - val_accuracy: 0.4000 - val_loss: 2.9249\n",
            "Epoch 6/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 0.0395 - val_accuracy: 0.4000 - val_loss: 3.1423\n",
            "Epoch 7/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 248ms/step - accuracy: 1.0000 - loss: 0.0317 - val_accuracy: 0.4000 - val_loss: 3.4201\n",
            "Epoch 8/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 229ms/step - accuracy: 1.0000 - loss: 0.0288 - val_accuracy: 0.4000 - val_loss: 3.2271\n",
            "Epoch 9/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 293ms/step - accuracy: 1.0000 - loss: 0.0251 - val_accuracy: 0.4000 - val_loss: 3.3974\n",
            "Epoch 10/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 251ms/step - accuracy: 1.0000 - loss: 0.0202 - val_accuracy: 0.4000 - val_loss: 3.7101\n",
            "Epoch 11/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 0.0170 - val_accuracy: 0.4000 - val_loss: 3.6995\n",
            "Epoch 12/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 251ms/step - accuracy: 1.0000 - loss: 0.0181 - val_accuracy: 0.4000 - val_loss: 3.6839\n",
            "Epoch 13/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 222ms/step - accuracy: 1.0000 - loss: 0.0153 - val_accuracy: 0.4000 - val_loss: 3.9885\n",
            "Epoch 14/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 1.0000 - loss: 0.0149 - val_accuracy: 0.4000 - val_loss: 3.9491\n",
            "Epoch 15/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 243ms/step - accuracy: 1.0000 - loss: 0.0131 - val_accuracy: 0.4000 - val_loss: 4.0269\n",
            "Epoch 16/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 249ms/step - accuracy: 1.0000 - loss: 0.0107 - val_accuracy: 0.4000 - val_loss: 4.1249\n",
            "Epoch 17/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 230ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 0.4000 - val_loss: 4.0158\n",
            "Epoch 18/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 293ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.4000 - val_loss: 3.9980\n",
            "Epoch 19/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 251ms/step - accuracy: 1.0000 - loss: 0.0119 - val_accuracy: 0.4000 - val_loss: 4.3712\n",
            "Epoch 20/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - accuracy: 1.0000 - loss: 0.0115 - val_accuracy: 0.4000 - val_loss: 4.1444\n",
            "Epoch 21/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 280ms/step - accuracy: 1.0000 - loss: 0.0114 - val_accuracy: 0.4000 - val_loss: 4.4497\n",
            "Epoch 22/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 237ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.4000 - val_loss: 4.1124\n",
            "Epoch 23/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 286ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.4000 - val_loss: 4.1021\n",
            "Epoch 24/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 236ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.4000 - val_loss: 4.1450\n",
            "Epoch 25/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 254ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 0.4000 - val_loss: 4.5776\n",
            "Epoch 26/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 251ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.4000 - val_loss: 4.6786\n",
            "Epoch 27/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.4000 - val_loss: 4.3111\n",
            "Epoch 28/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 301ms/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.4000 - val_loss: 4.7407\n",
            "Epoch 29/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 230ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.4000 - val_loss: 4.3002\n",
            "Epoch 30/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 257ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.4000 - val_loss: 4.8793\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d12576f3610>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the Model"
      ],
      "metadata": {
        "id": "iso1uTQAop6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
        "print('\\nTest Accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPUZVFEYkLY8",
        "outputId": "c7e45d93-4ba8-4bfa-b4c1-b236c57d46b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 - 18s - 882ms/step - accuracy: 0.4000 - loss: 4.9369\n",
            "\n",
            "Test Accuracy: 0.4000000059604645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SOLUTION:\n",
        "\n",
        "To address the problem, we adapted the methodology from the original soybean disease identification paper by **substituting** the dataset with potato images and adjusting the image size to **150x150 pixels**.\n",
        "\n",
        "We utilized the **DenseNet201** model with all base layers set to non-trainable, trained it for **30 epochs**, and tailored the model for potato disease classification.\n",
        "\n",
        "These modifications allowed us to effectively apply transfer learning techniques to a new crop context, demonstrating the model's versatility and efficacy in identifying potato diseases."
      ],
      "metadata": {
        "id": "lUq8v-2uytpY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}